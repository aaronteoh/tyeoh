{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from WikiVoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "# https://en.wikivoyage.org/wiki/Wikivoyage:Database_dump\n",
    "url = 'https://dumps.wikimedia.org/enwikivoyage/latest/enwikivoyage-latest-pages-articles.xml.bz2'\n",
    "if not os.path.isfile(url.split('/')[-1]):\n",
    "    #this step takes some time\n",
    "    urllib.urlretrieve (url, url.split('/')[-1])\n",
    "\n",
    "# https://stackoverflow.com/questions/22676/how-do-i-download-a-file-over-http-using-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bz2 import BZ2Decompressor\n",
    "\n",
    "decompressed = ('.').join(url.split('/')[-1].split('.')[:-1])\n",
    "\n",
    "if not os.path.isfile(decompressed):\n",
    "    with open(decompressed, 'wb') as new_file, open(url.split('/')[-1], 'rb') as file:\n",
    "        decompressor = BZ2Decompressor()\n",
    "        #this step takes some time\n",
    "        for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "            new_file.write(decompressor.decompress(data))\n",
    "        \n",
    "# https://stackoverflow.com/questions/16963352/decompress-bz2-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert XML to dict, process to ignore redirects and get articles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xmltodict\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "import xmltodict\n",
    "\n",
    "from collections import defaultdict\n",
    "completed =0\n",
    "articles = defaultdict(list)\n",
    "\n",
    "\n",
    "if not os.path.isfile('wikivoyage_latest_articles_text.json'):\n",
    "    #this step takes some time\n",
    "    with open('enwikivoyage-latest-pages-articles.xml') as fd:\n",
    "        doc = xmltodict.parse(fd.read())\n",
    "\n",
    "    data = doc['mediawiki']['page']\n",
    "    print('To process %s records' %len(data))\n",
    "\n",
    "    for item in data:\n",
    "        if 'redirect' not in item:\n",
    "            try:\n",
    "                articles[item['title']].append(item['revision']['text']['#text'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        completed+=1\n",
    "        if completed%10000==0 or completed==len(data):\n",
    "            print('Completed %s' %completed)\n",
    "\n",
    "    print(len(articles))\n",
    "    for article, text in articles.iteritems():\n",
    "        articles[article] = \"\".join(text)\n",
    "\n",
    "    with open('wikivoyage_latest_articles_text.json', 'w') as f:\n",
    "        json.dump(articles, f)\n",
    "\n",
    "    del doc\n",
    "    del data\n",
    "    del articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Extract lat long and parent article from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "with open('wikivoyage_latest_articles_text.json', 'r') as f:\n",
    "    consolidated = json.load(f)\n",
    "\n",
    "print(len(consolidated))\n",
    "\n",
    "cleaned = {}\n",
    "completed = 0\n",
    "issues = 0\n",
    "\n",
    "for article_name in consolidated:\n",
    "#   ignore articles which are not destinations (from article name and article tags)\n",
    "    if not article_name.startswith('Module') and not article_name.startswith('Template:') and not article_name.startswith('Category:')\\\n",
    "    and not article_name.startswith('File:') and not article_name.startswith('Wikivoyage:') and not article_name.startswith('MediaWiki:') and not article_name in ['Moon', 'Space']\\\n",
    "        and len(re.findall('{{outlinetopic}}', consolidated[article_name].lower()))==0 and len(re.findall('{{usabletopic}}', consolidated[article_name].lower()))==0\\\n",
    "        and len(re.findall('{{guidetopic}}', consolidated[article_name].lower())) == 0 and len(re.findall('{{startopic}}', consolidated[article_name].lower()))==0\\\n",
    "        and len(re.findall('{{disamb}}', consolidated[article_name].lower()))==0 and len(re.findall('{{disambig}}', consolidated[article_name].lower()))==0 and len(re.findall('{{disambiguation}}', consolidated[article_name].lower()))==0\\\n",
    "        and len(re.findall('{{itinerary}}', consolidated[article_name].lower()))==0 \\\n",
    "        and len(re.findall('{{usablephrasebook}}', consolidated[article_name].lower()))==0 and len(re.findall('{{phrasebookguide}}', consolidated[article_name].lower()))==0 \\\n",
    "        and len(re.findall('{{Title-Index page}}', consolidated[article_name]))==0 \\\n",
    "        and len(re.findall('{{GalleryPageOf.*}}', consolidated[article_name]))==0 \\\n",
    "        and len(re.findall('{{stub}}', consolidated[article_name].lower())) == 0 \\\n",
    "        and len(re.findall('{{historical}}', consolidated[article_name].lower())) == 0:\n",
    "\n",
    "\n",
    "        IsPartOf = re.findall('{{IsPartOf.*}}', consolidated[article_name]) + re.findall('{{isPartOf.*}}', consolidated[article_name])\n",
    "\n",
    "\n",
    "        geo = re.findall('{{geo.*}}', consolidated[article_name].lower())\n",
    "\n",
    "\n",
    "        city = re.findall('{{usablecity}}', consolidated[article_name].lower()) + re.findall('{{outlinecity}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{guidecity}}', consolidated[article_name].lower()) + re.findall('{{starcity}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{ussblecity}}', consolidated[article_name].lower())\n",
    "\n",
    "        country = re.findall('{{usablecountry}}', consolidated[article_name].lower()) + re.findall('{{outlinecountry}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{guidecountry}}', consolidated[article_name].lower()) + re.findall('{{starcountry}}', consolidated[article_name].lower())\n",
    "\n",
    "        district = re.findall('{{usabledistrict}}', consolidated[article_name].lower()) + re.findall('{{outlinedistrict}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{guidedistrict}}', consolidated[article_name].lower())+ re.findall('{{stardistrict}}', consolidated[article_name].lower())\n",
    "\n",
    "        region = re.findall('{{usableregion}}', consolidated[article_name].lower()) + re.findall('{{outlineregion}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{guideregion}}', consolidated[article_name].lower()) + re.findall('{{extraregion\\|subregion=yes}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{starregion}}', consolidated[article_name].lower()) + re.findall('{{extraregion\\|subregion=no}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{extraregion}}', consolidated[article_name].lower())\n",
    "\n",
    "        airport = re.findall('{{usableairport}}', consolidated[article_name].lower()) + re.findall('{{outlineairport}}', consolidated[article_name].lower())\\\n",
    "                    + re.findall('{{guideairport}}', consolidated[article_name].lower())\n",
    "\n",
    "        park = re.findall('{{usablepark}}', consolidated[article_name].lower()) + re.findall('{{outlinepark}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{guidepark}}', consolidated[article_name].lower()) + re.findall('{{starpark}}', consolidated[article_name].lower())\n",
    "\n",
    "        diveguide = re.findall('{{usablediveguide}}', consolidated[article_name].lower()) + re.findall('{{outlinediveguide}}', consolidated[article_name].lower()) \\\n",
    "                    + re.findall('{{guidediveguide}}', consolidated[article_name].lower()) + re.findall('{{stardiveguide}}', consolidated[article_name].lower())\n",
    "\n",
    "        continent = re.findall('{{usablecontinent}}', consolidated[article_name].lower()) + re.findall('{{outlinecontinent}}', consolidated[article_name].lower())\n",
    "\n",
    "\n",
    "        if len(geo)>0 and len(diveguide)==0 and article_name not in ['Commonwealth of Independent States']: #skip dive guides\n",
    "            article_name = article_name.replace('_', ' ').split('{{')[0].strip().lower()\n",
    "            \n",
    "            if unicodedata.normalize('NFKD', article_name).encode('ascii', 'ignore') == 'brac':\n",
    "                article_name = 'brac'\n",
    "            elif unicodedata.normalize('NFKD', article_name).encode('ascii', 'ignore') == 'rugen':\n",
    "                article_name = 'rugen'\n",
    "            \n",
    "            cleaned[article_name] = {}\n",
    "\n",
    "            # get lat long\n",
    "            if len(geo)>0:\n",
    "                cleaned[article_name]['latitude'] = geo[-1].split('|')[1]\n",
    "                cleaned[article_name]['longitude'] = geo[-1].split('|')[2]\n",
    "\n",
    "            # get parents\n",
    "            cleaned[article_name]['ispartof'] = []\n",
    "            for parts in IsPartOf:\n",
    "                parent = parts.split('|')[1].replace('}','').replace('_', ' ').split('{{')[0].strip().lower()\n",
    "\n",
    "                \n",
    "                #fixes for inconsistent data\n",
    "                if parent == 'ko pha ngan':\n",
    "                    parent = 'ko pha-ngan'\n",
    "                elif parent in ['lowland shandong', 'highland shandong', 'coastal shandong']:\n",
    "                    parent = 'shandong'\n",
    "                elif parent in ['southern delaware', 'northern delaware', 'central delaware']:\n",
    "                    parent = 'delaware'\n",
    "                elif parent in ['burgraviate', 'puster valley', 'eisack valley']:\n",
    "                    parent = 'south tyrol'\n",
    "                elif parent == 'bohemian-moravian highlands':\n",
    "                    parent = 'highlands (czech republic)'\n",
    "                elif parent == 'brahmanbaria district':\n",
    "                    parent = 'chittagong division'\n",
    "                elif parent == 'eastern desert':\n",
    "                    parent = 'eastern desert (jordan)'\n",
    "                elif parent == 'caribbean coast':\n",
    "                    parent = 'caribbean coast (guatemala)'\n",
    "                elif parent == 'santander (colombia)':\n",
    "                    parent = 'santander (department, colombia)'\n",
    "                elif parent == 'tripolitania':\n",
    "                    parent = 'libya'\n",
    "                elif parent == 'wooster area ohio':\n",
    "                    parent = 'wooster area'\n",
    "                elif parent == 'tatra mountains (poland)':\n",
    "                    parent = 'tatra national park (poland)'\n",
    "                elif parent == 'salcette':\n",
    "                    parent = 'salcete'\n",
    "                elif parent == 'eastern barbados':\n",
    "                    parent = 'central eastern barbados'\n",
    "                elif parent == 'east khasi hills':\n",
    "                    parent = 'meghalaya'\n",
    "                elif parent == 'samar':\n",
    "                    parent = 'samar (philippines)'\n",
    "                elif parent == 'chikmagalur (district)' and article_name != 'chikmagalur' :\n",
    "                    parent = 'chikmagalur'\n",
    "                elif unicodedata.normalize('NFKD', parent).encode('ascii', 'ignore') == 'rugen':\n",
    "                    parent = 'rugen'\n",
    "                elif article_name == 'chikmagalur':\n",
    "                    parent = 'karnataka'\n",
    "        \n",
    "                cleaned[article_name]['ispartof'].append(parent)\n",
    "                \n",
    "                \n",
    "            # get destination type\n",
    "            if len(airport)>0:\n",
    "                cleaned[article_name]['type']='airport'\n",
    "            elif len(city)>0:\n",
    "                cleaned[article_name]['type']='city'\n",
    "            elif len(continent)>0:\n",
    "                cleaned[article_name]['type']='continent'\n",
    "            elif len(country)>0:\n",
    "                cleaned[article_name]['type']='country'\n",
    "            elif len(district)>0:\n",
    "                cleaned[article_name]['type']='district'\n",
    "            elif len(park)>0:\n",
    "                cleaned[article_name]['type']='park'\n",
    "            elif len(region)>0:\n",
    "                cleaned[article_name]['type']='region'\n",
    "\n",
    "\n",
    "\n",
    "    completed +=1\n",
    "    if completed%1000==0 or completed==len(consolidated):\n",
    "        print('Completed: %s' %completed)\n",
    "\n",
    "\n",
    "print('Total sorted: %s' %len(cleaned))\n",
    "\n",
    "with open('destination_details.json', 'w') as f:\n",
    "    json.dump(cleaned, f)\n",
    "\n",
    "del consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map out parent child relationship for all articles into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('destination_details.json', 'r') as f:\n",
    "    cleaned = json.load(f)\n",
    "\n",
    "\n",
    "def map_destinations(mapped, current_dict):\n",
    "    global destination_mapping, parent, destination\n",
    "    if not mapped:\n",
    "        if parent in current_dict:\n",
    "            current_dict[parent].update({destination: {}})\n",
    "            mapped = True\n",
    "            \n",
    "            #find if any of the heads match this article\n",
    "            if destination in destination_mapping:\n",
    "                current_dict[parent][destination] = destination_mapping.pop(destination)\n",
    "        \n",
    "        #can only happen at top level\n",
    "        elif destination in current_dict and current_dict==destination_mapping:\n",
    "            current_dict[parent] = {}\n",
    "            current_dict[parent][destination] = current_dict.pop(destination)\n",
    "            mapped = True\n",
    "            attached = False\n",
    "            #find if any of the tails match this parent\n",
    "            step_through_dict(attached, destination_mapping)\n",
    "\n",
    "        else:\n",
    "            for next_level in current_dict:\n",
    "                mapped, current_dict[next_level] = map_destinations(mapped, current_dict[next_level])\n",
    "    \n",
    "    return mapped, current_dict\n",
    "\n",
    "\n",
    "def step_through_dict(attached, current_dict):\n",
    "    global destination_mapping, destination, parent\n",
    "\n",
    "    iter_list = current_dict.keys()\n",
    "    for item in iter_list:\n",
    "        if not attached:\n",
    "            if item == parent and current_dict!=destination_mapping:\n",
    "                current_dict[parent][destination] = destination_mapping.pop(parent)[destination]\n",
    "                attached = True\n",
    "\n",
    "            elif len(current_dict[item])>0:\n",
    "                attached = step_through_dict(attached, current_dict[item])\n",
    "\n",
    "    return attached\n",
    "        \n",
    "    \n",
    "destination_mapping = {}\n",
    "print('To process %s records' %len(cleaned))\n",
    "processed = 0\n",
    "for destination in cleaned:\n",
    "    for parent in cleaned[destination]['ispartof']:\n",
    "            \n",
    "        mapped = False\n",
    "        mapped, destination_mapping = map_destinations(mapped, destination_mapping)\n",
    "        \n",
    "        if not mapped:\n",
    "            destination_mapping[parent] = {}\n",
    "            destination_mapping[parent][destination] = {}\n",
    "    processed+=1\n",
    "    if processed%1000==0 or processed==len(cleaned):\n",
    "        print('Completed: %s' %processed)\n",
    "\n",
    "with open('destination_mapping.json', 'w') as f:\n",
    "    json.dump(destination_mapping, f)\n",
    "    \n",
    "del destination_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data and return to previous step to fix inconsistent spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('destination_mapping.json', 'r') as f:\n",
    "    destination_mapping = json.load(f)\n",
    "for item in destination_mapping:\n",
    "    print item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrive parent chain for destination input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "destination = 'thailand'\n",
    "\n",
    "with open('destination_details.json', 'r') as f:\n",
    "    details = json.load(f)\n",
    "\n",
    "\n",
    "def get_parent(current, chain=''):\n",
    "    if chain is '':\n",
    "        chain=current.lower()\n",
    "        current=current.lower()\n",
    "    try:\n",
    "        for parent in details[current]['ispartof']:\n",
    "            chain = '%s|%s' %(parent, chain)\n",
    "            chain = get_chain(parent, chain)\n",
    "    except KeyError:\n",
    "        return chain\n",
    "    else:\n",
    "        return chain\n",
    "print get_parent(destination)\n",
    "\n",
    "del details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get child articles from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('destination_details.json', 'r') as f:\n",
    "    details = json.load(f)\n",
    "    \n",
    "def get_child(search):\n",
    "    child_articles = []\n",
    "    for article in details:\n",
    "        for parent in details[article]['ispartof']:\n",
    "            if parent == search.lower():\n",
    "                child_articles.append(article)\n",
    "    return child_articles\n",
    "for item in get_child('thailand'):\n",
    "    print item\n",
    "del details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search destinations containing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('destination_details.json', 'r') as f:\n",
    "    details = json.load(f)\n",
    "    \n",
    "def search(input):\n",
    "    results = []\n",
    "    for item in details:\n",
    "        if input.lower() in item:\n",
    "            results.append(item)\n",
    "    return results\n",
    "\n",
    "for result in search('bugis'):\n",
    "    print result\n",
    "    print get_parent(result)\n",
    "    print get_child(result)\n",
    "del details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
