{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input files for this notebook are output files of the destinations graph notebook so it'll be necessary to complete that first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load partially cleaned data from destinations graph notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Get the text from each article (Step 4 in destinations graph notebook)\n",
    "with open('wikivoyage_latest_articles_text.json', 'r') as f:\n",
    "    raw_text = json.load(f)\n",
    "\n",
    "# Get list of destinations to process (Step 5 in destinations graph notebook)\n",
    "with open('destination_details.json', 'r') as f:\n",
    "    destination_details = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Names in raw_text are unedited while those in destinations_to_check are cleaned, need to standardize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def standardize_name(destination_name):\n",
    "    # this is following the name cleaning process in step 5 of destinations graph notebook\n",
    "    destination_name = destination_name.replace('_', ' ').split('{{')[0].strip().lower()\n",
    "    if unicodedata.normalize('NFKD', destination_name).encode('ascii', 'ignore') == 'brac':\n",
    "        destination_name = 'brac'\n",
    "    elif unicodedata.normalize('NFKD', destination_name).encode('ascii', 'ignore') == 'rugen':\n",
    "        destination_name = 'rugen'\n",
    "    return destination_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract specific parts of article text for more efficient extraction of themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary with destination name as key and the relevant text as value\n",
    "consolidated = {}\n",
    "\n",
    "import re\n",
    "print('To process %s records.' %len(raw_text))\n",
    "\n",
    "for i in raw_text:\n",
    "    if standardize_name(i) in destination_details:\n",
    "        \n",
    "        \n",
    "        # get the section of introductory text which might have indication of what a destination is known for. \n",
    "        # this section is before the 'Get in' section\n",
    "        t =''\n",
    "        if '==Get in==' in raw_text[i]:\n",
    "            t = raw_text[i].split('==Get in==')[0]\n",
    "            t += '\\n'\n",
    "        elif '== Get in ==' in raw_text[i]:\n",
    "            t = raw_text[i].split('== Get in ==')[0]\n",
    "            t += '\\n'\n",
    "\n",
    "            \n",
    "        # next, the 'See' and 'Do' sections are the mostly likely places to find information on the destination's themes\n",
    "        if '==See==' in raw_text[i]:\n",
    "            t+= raw_text[i].split('==See==')[1].split('==Do==')[0].split('== Do ==')[0].split('==Eat==')[0].split('== Eat ==')[0].split('==Eat and Drink==')[0].split('== Eat and Drink==')[0].split('==Buy==')[0].split('== Buy ==')[0]\n",
    "        elif '== See ==' in raw_text[i]:\n",
    "            t += raw_text[i].split('== See ==')[1].split('==Do==')[0].split('== Do ==')[0].split('==Eat==')[0].split('== Eat ==')[0].split('==Eat and Drink==')[0].split('== Eat and Drink==')[0].split('==Buy==')[0].split('== Buy ==')[0]\n",
    "        if '==Do==' in raw_text[i]:\n",
    "            t += raw_text[i].split('==Do==')[1].split('==Eat==')[0].split('== Eat ==')[0].split('==Eat and Drink==')[0].split('== Eat and Drink==')[0].split('==Buy==')[0].split('== Buy ==')[0]\n",
    "        if '== Do ==' in raw_text[i]:\n",
    "            t += raw_text[i].split('== Do ==')[1].split('==Eat==')[0].split('== Eat ==')[0].split('==Eat and Drink==')[0].split('== Eat and Drink==')[0].split('==Buy==')[0].split('== Buy ==')[0]\n",
    "        \n",
    "        \n",
    "        # if there is text found in either the introductory part, or under 'See' or 'Do', proceed with further cleaning\n",
    "        if t!='':\n",
    "            # remove tags, headers, links\n",
    "            t = re.sub('{{.*?}}', '', t)\n",
    "            t = re.sub('===.*?===', '', t)\n",
    "            t = re.sub('==.*?==', '', t)\n",
    "            t = re.sub('<!--.*?-->', '', t)\n",
    "            while len(re.findall('\\[\\[.*?\\|', t))>0:\n",
    "                t = re.sub('\\[\\[.*?\\|', '[[', t)\n",
    "            t = re.sub('\\[\\[.*?\\]\\]\\n', '\\n', t)\n",
    "            t = t.replace('[', '').replace(']', '').replace(\"'''\", \"\").replace(\"''\", \"\")\n",
    "\n",
    "            t = re.sub('\\(.*?\\)', '', t)\n",
    "            t = re.sub('\\*.*?\\n', '', t)\n",
    "            t = re.sub('\\|.*?\\n', '', t)\n",
    "            t = re.sub('{{\\w*', '', t)\n",
    "            # t = re.sub('File:.*?\\n', '', t)\n",
    "            t = t.replace('}}', '').replace('::','')\n",
    "            t = re.sub(r'https://.*?[\\s|\\n]', '', t)\n",
    "            t = re.sub(r'http://.*?[\\s|\\n]', '', t)\n",
    "            \n",
    "            # replace double linebreaks and double spaces with singles\n",
    "            while '\\n\\n' in t:\n",
    "                t = t.replace('\\n\\n','\\n')\n",
    "            while '  ' in t:\n",
    "                t = t.replace('  ', ' ')\n",
    "                \n",
    "            # remove whitespaces\n",
    "            t = t.strip()\n",
    "            compiled = ''\n",
    "            rows = t.split('\\n')\n",
    "            \n",
    "            # only consider lines with more than 6 words before a linebreak (if fewer than that likely irrelevant)\n",
    "            for row in rows:\n",
    "                if len(row.split(' '))>6:\n",
    "                    compiled+=row\n",
    "                    compiled+=' '\n",
    "            compiled = compiled.strip()\n",
    "            \n",
    "            # only add destinations with more than 20 words in total to the final output for themes extraction\n",
    "            if len(compiled.split(' '))>20:\n",
    "                consolidated[standardize_name(i)] = compiled\n",
    "            if len(consolidated)%1000==0:\n",
    "                print 'Completed %s' %len(consolidated)\n",
    "\n",
    "# dump output into a file\n",
    "with open('cleaned_text.json', 'w') as f:\n",
    "    json.dump(consolidated, f)\n",
    "print('Final output %s records.' %len(consolidated))\n",
    "\n",
    "del t\n",
    "del compiled\n",
    "del consolidated\n",
    "del raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('cleaned_text.json', 'r') as f:\n",
    "    cleaned_text = json.load(f)\n",
    "\n",
    "corpus = []\n",
    "id_lookup = {}\n",
    "completed = 0\n",
    "for destination in cleaned_text:\n",
    "    id_lookup[len(corpus)] = destination\n",
    "    corpus.append(cleaned_text[destination])\n",
    "\n",
    "print('Corpus size: %s' %len(corpus))\n",
    "print('Lookup size: %s' %len(id_lookup))\n",
    "\n",
    "with open('corpus_id_lookup.json', 'w') as f:\n",
    "    json.dump(id_lookup, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Perform TFIDF on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0.001, max_df=0.6, lowercase=False, use_idf=False, norm=None, stop_words = 'english')\n",
    "\n",
    "tfidf_matrix =  tf.fit_transform(corpus)\n",
    "feature_names = tf.get_feature_names()\n",
    "print 'Number of unique words in corpus: %s '%len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Transform the output to store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completed=0\n",
    "doc_id = 0\n",
    "consolidated = {}\n",
    "for doc in tfidf_matrix.todense():\n",
    "    word_id = 0\n",
    "    consolidated[id_lookup[doc_id]] = {}\n",
    "    for score in doc.tolist()[0]:\n",
    "        if score > 0:\n",
    "            consolidated[id_lookup[doc_id]][feature_names[word_id].encode(\"utf-8\")] = score\n",
    "        word_id +=1\n",
    "    doc_id +=1\n",
    "    if len(consolidated)%1000==0:\n",
    "        print 'Completed %s' %len(consolidated)\n",
    "\n",
    "with open('tfidf_scores.json', 'w') as f:\n",
    "    json.dump(consolidated, f)\n",
    "    \n",
    "del consolidated\n",
    "del tfidf_matrix\n",
    "del feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Get themes from word prominence in destination articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "themes_dict = {\n",
    "    'beach': ['beach', 'beaches'],\n",
    "    'shopping': ['shopping', 'malls'],\n",
    "    'temples': ['temple', 'temples'],\n",
    "    'surfing': ['surf', 'surfing', 'surfers'],\n",
    "    'diving': ['dive', 'diving', 'divers'],\n",
    "    'hiking': ['hike', 'hiking', 'hikers', 'trek', 'trekking', 'trekkers'],\n",
    "    # 'family': ['families', 'kids', 'children', 'seniors'],\n",
    "    'culture': ['culture', 'cultural', 'cultures'],\n",
    "    'food': ['foodie', 'food', 'restaurants', 'delicacy', 'delicacies'],\n",
    "    'museums': ['museums', 'museum'],\n",
    "}\n",
    "\n",
    "with open('tfidf_scores.json', 'r') as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "consolidated = {'beach': [], 'shopping': [], 'temples': [], 'surfing': [], 'diving': [], 'hiking': [], 'culture': [], 'food': [], 'museums': []}\n",
    "completed=0\n",
    "for destination in data:\n",
    "    for theme in themes_dict:\n",
    "        score = 0\n",
    "        for word in themes_dict[theme]:\n",
    "            if word in data[destination]:\n",
    "                score+=data[destination][word]\n",
    "        if score>0:\n",
    "            consolidated[theme].append((destination, score))\n",
    "    completed+=1\n",
    "    if completed%1000==0:\n",
    "        print('Completed %s/%s' %(completed, len(data)))\n",
    "\n",
    "with open('destination_themes.json', 'w') as f:\n",
    "    json.dump(consolidated, f)\n",
    "    \n",
    "del data\n",
    "del consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Re use functions created in destinations graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_parent(current, chain=''):\n",
    "    if chain is '':\n",
    "        chain=current.lower()\n",
    "        current=current.lower()\n",
    "    try:\n",
    "        for parent in destination_details[current]['ispartof']:\n",
    "            chain = '%s|%s' %(parent, chain)\n",
    "            chain = get_parent(parent, chain)\n",
    "    except KeyError:\n",
    "        return chain\n",
    "    else:\n",
    "        return chain\n",
    "print get_parent('Thailand')\n",
    "\n",
    "def get_child(search):\n",
    "    child_articles = []\n",
    "    for article in destination_details:\n",
    "        for parent in destination_details[article]['ispartof']:\n",
    "            if parent == search.lower():\n",
    "                child_articles.append(article)\n",
    "    return child_articles\n",
    "\n",
    "print get_child('Thailand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Get top destinations for given theme and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('destination_themes.json', 'r') as f:\n",
    "    destination_themes = json.load(f)\n",
    "    \n",
    "theme = 'beach'\n",
    "region = 'Asia'\n",
    "sorted_scores = sorted(destination_themes[theme], key=lambda t: t[1] * -1)\n",
    "\n",
    "printed = 0\n",
    "for score in sorted_scores:\n",
    "    if region.lower() in get_parent(score[0]) and (len(get_child(score[0]))==0):\n",
    "        print '%s (%s): %s' %(score[0].title(), score[1], get_parent(score[0]).title().replace('|', ' > '))\n",
    "        printed +=1\n",
    "        if printed == 5:\n",
    "            break\n",
    "\n",
    "del destination_themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Get top themes for a given destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('destination_themes.json', 'r') as f:\n",
    "    destination_themes = json.load(f)\n",
    "    \n",
    "destination = 'paris'\n",
    "final_themes = []\n",
    "for theme in destination_themes:\n",
    "    for destination_score in destination_themes[theme]:\n",
    "        if destination.lower() == destination_score[0]:\n",
    "            final_themes.append((theme, destination_score[1]))\n",
    "sorted_scores = sorted(final_themes, key=lambda t: t[1] * -1)\n",
    "\n",
    "print get_parent(destination.lower()).title().replace('|', ' > ')\n",
    "print\n",
    "for item in sorted_scores:\n",
    "    print '%s (%s)' %(item[0].title(), item[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
